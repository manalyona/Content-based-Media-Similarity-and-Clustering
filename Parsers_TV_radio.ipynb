{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from math import ceil\n",
    "from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from datetime import date, datetime\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Chanell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "times = [['000000', '080000'], ['080000', '160000'],['160000', '235959']]\n",
    "tv1_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        tv1_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        for time in times:\n",
    "            try:\n",
    "                url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.1tv.ru%20OR%20domainis:1tv.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                tv1_20[f'2020.{month:02d}.{day+1:02d}'].extend([a['href'] for a in soup.body.table.find_all('a')])\n",
    "                print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))\n",
    "            except (AttributeError, IndexError):\n",
    "                print(date)\n",
    "with open('tv1_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(tv1_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv1_2020 = []\n",
    "for date in tqdm_notebook(list(tv1_20.keys())):\n",
    "    for link in tqdm_notebook(tv1_20[date]):\n",
    "#         if link not in pd.DataFrame(tv1_2020).Link:\n",
    "        try:\n",
    "            soup = BeautifulSoup(requests.get(link).text)\n",
    "            tv1_2020.append({'Media':'1tv.ru', 'Date':date, 'Link':link, \n",
    "                             'Text':' '.join([soup.h1.text,\n",
    "                                              ' '.join([p.text for p in soup\n",
    "                                                        .find('div', 'editor text-block active')\n",
    "                                                        .find_all('p')])]).replace('\\xa0', ' '), \n",
    "                             'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "            IndexError, requests.exceptions.TooManyRedirects, ValueError, TypeError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(tv1_2020).to_csv('tv1_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(tv1_2020).to_csv('tv1_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(tv1_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "ntv_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        ntv_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        try:\n",
    "            url = f'https://www.ntv.ru/rest/news/day.jsp?dt={date}'\n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.text)\n",
    "            ntv_20[f'2020.{month:02d}.{day+1:02d}'] = ['https://www.ntv.ru'+a['href'] for a in soup.find('div', 'day_news').find_all('a') if 'novosti' in a['href']]\n",
    "            print(f'2020.{month:02d}.{day+1:02d}', len(ntv_20[f'2020.{month:02d}.{day+1:02d}']))\n",
    "        except (AttributeError, IndexError):\n",
    "            print(date)\n",
    "with open('ntv_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(ntv_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntv_2020 = []\n",
    "for date in tqdm_notebook(list(ntv_20.keys())):\n",
    "    for link in tqdm_notebook(ntv_20[date]):\n",
    "#         if link not in pd.DataFrame(echo_2020).Link:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            soup = BS(r.text)\n",
    "            ntv_2020.append({'Media':'ntv.ru', 'Date':date, 'Link':link,\n",
    "                                  'Text':' '.join([soup.h1.text, ' '.join([p.text for p in soup.find('div', 'inpagebody').find_all('p')])]).replace('\\xa0', ' '), \n",
    "                                  'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(ntv_2020).to_csv('ntv_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(ntv_2020).to_csv('ntv_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(ntv_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zvezda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "times = [['000000', '120000'], ['120000', '235959']]\n",
    "zvezda_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        zvezda_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        for time in times:\n",
    "            try:\n",
    "                url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.tvzvezda.ru%20OR%20domainis:tvzvezda.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "#                 url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.mk.ru%20OR%20domainis:mk.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "    #             url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.kp.ru%20OR%20domainis:kp.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                zvezda_20[f'2020.{month:02d}.{day+1:02d}'].extend([a['href'] for a in soup.body.table.find_all('a')])\n",
    "                print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))\n",
    "            except (AttributeError, IndexError):\n",
    "                print(date)\n",
    "with open('zvezda_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(zvezda_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvzvezda_2020 = []\n",
    "for date in tqdm_notebook(list(zvezda_20.keys())):\n",
    "    for link in tqdm_notebook(zvezda_20[date]):\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            soup = BeautifulSoup(r.text)\n",
    "            tvzvezda_2020.append({'Media':'tvzvezda.ru', 'Date':date, 'Link':link,\n",
    "                                  'Text':' '.join([soup.h1.text, ' '.join([p.text for p in soup.find('div', 'glav_text').find_all('p')])]), \n",
    "                                  'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(tvzvezda_2020).to_csv('tvzvezda_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(tvzvezda_2020).to_csv('tvzvezda_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(tvzvezda_2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dozhd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "tvrain_20 = dict()\n",
    "year = '2020'\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "#         date = f'2020{month:02d}{day+1:02d}'\n",
    "        links_temp = list()\n",
    "        url = f'https://tvrain.ru/archive/?search_teleshow_cat=&search_year={year}&search_month={month}&search_day={day+1}&query=&page={page+1}'\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text)\n",
    "        num_of_pages = ceil(int(soup.find('a', 'filter__items__item--label_brand').text.strip().split()[-1])/20)\n",
    "        print('num_of_news', int(soup.find('a', 'filter__items__item--label_brand').text.strip().split()[-1]),\n",
    "             'num_of pages', num_of_pages)\n",
    "        for page in range(num_of_pages):\n",
    "            try:\n",
    "                url = f'https://tvrain.ru/archive/?search_teleshow_cat=&search_year={year}&search_month={month}&search_day={day}&query=&page={page+1}'\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                links_temp.extend(['https://tvrain.ru'+a['href'] for a in soup.find_all('a', 'chrono_list__item__info__name')])\n",
    "                print(f'2020.{month:02d}.{day+1:02d}', len(links_temp))\n",
    "#                 page+=1\n",
    "            except (AttributeError, IndexError):\n",
    "                print(date)\n",
    "#             if len(links_temp) == number:\n",
    "#                 break\n",
    "        tvrain_20[f'2020.{month:02d}.{day+1:02d}'] = links_temp\n",
    "        \n",
    "with open('tvrain_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(tvrain_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvrain_2020 = []\n",
    "for date in tqdm_notebook(list(tvrain_20.keys())):\n",
    "    for link in tqdm_notebook(tvrain_20[date]):\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            soup = BS(r.text)\n",
    "            tvrain_2020.append({'Media':'tvrain.ru', 'Date':date, 'Link':link,\n",
    "                                  'Text':' '.join([soup.h1.text, ' '.join([p.text for p in soup.find('div', id='article_content_text').find_all('p') if 'донейт' not in p.text])]).replace('\\xa0', ' ').replace('\\n\\n', ' '), \n",
    "                                  'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(tvrain_2020).to_csv('tvrain_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(tvrain_2020).to_csv('tvrain_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(tvrain_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Govorit Moskva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "times = [['000000', '120000'], ['120000', '235959']]\n",
    "govoritmoskva_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        govoritmoskva_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        for time in times:\n",
    "            try:\n",
    "                url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.govoritmoskva.ru%20OR%20domainis:govoritmoskva.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                govoritmoskva_20[f'2020.{month:02d}.{day+1:02d}'].extend([a['href'] for a in soup.body.table.find_all('a')])\n",
    "                print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))\n",
    "            except (AttributeError, IndexError):\n",
    "                print(date)\n",
    "with open('govoritmoskva_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(govoritmoskva_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "govoritmoskva_2020 = []\n",
    "for date in tqdm_notebook(list(govoritmoskva_20.keys())):\n",
    "    for link in tqdm_notebook(govoritmoskva_20[date]):\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            soup = BeautifulSoup(r.text)\n",
    "            govoritmoskva_2020.append({'Media':'govoritmoskva.ru', 'Date':date, 'Link':link,\n",
    "                                  'Text':' '.join([soup.h1.text, ' '.join([p.text for p in soup.find('div', 'textContent').find_all('p')]).strip()]), \n",
    "                                  'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(govoritmoskva_2020).to_csv('govoritmoskva_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(govoritmoskva_2020).to_csv('govoritmoskva_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(govoritmoskva_2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Echo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "times = [['000000', '120000'], ['120000', '235959']]\n",
    "echo_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        echo_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        for time in times:\n",
    "            try:\n",
    "                url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.echo.msk.ru%20OR%20domainis:echo.msk.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                echo_20[f'2020.{month:02d}.{day+1:02d}'].extend([a['href'] for a in soup.body.table.find_all('a')])\n",
    "                print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))\n",
    "            except (AttributeError, IndexError):\n",
    "                print(date)\n",
    "with open('echo_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(echo_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo_2020 = []\n",
    "for date in tqdm_notebook(list(echo_20.keys())):\n",
    "    for link in tqdm_notebook(echo_20[date]):\n",
    "        if 'comments' not in link and 'programs' not in link:\n",
    "            try:\n",
    "                r = requests.get(link)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                echo_2020.append({'Media':'echo.msk.ru', 'Date':date, 'Link':link,\n",
    "                                      'Text':' '.join([soup.h1.text, ' '.join([p.text.strip() for p in soup.find('div', 'typical').find_all('p')])]), \n",
    "                                      'Dep':'', 'Tags':[]})\n",
    "            except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                    IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "                print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(echo_2020).to_csv('echo_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(echo_2020).to_csv('echo_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(echo_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Svoboda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "times = [['000000', '120000'], ['120000', '235959']]\n",
    "svoboda_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        svoboda_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        for time in times:\n",
    "            try:\n",
    "                url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.svoboda.org%20OR%20domainis:svoboda.org)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                svoboda_20[f'2020.{month:02d}.{day+1:02d}'].extend([a['href'] for a in soup.body.table.find_all('a')])\n",
    "                print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))\n",
    "            except (AttributeError, IndexError):\n",
    "                print(date)\n",
    "with open('svoboda_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(svoboda_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svoboda_2020 = []\n",
    "for date in tqdm_notebook(list(svoboda_20.keys())):\n",
    "    for link in tqdm_notebook(svoboda_20[date]):\n",
    "#         if link not in pd.DataFrame(echo_2020).Link:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            soup = BeautifulSoup(r.text)\n",
    "            svoboda_2020.append({'Media':'svoboda.org', 'Date':date, 'Link':link,\n",
    "                                  'Text':' '.join([soup.h1.text.strip(), ' '.join([p.text for p in soup.find('div', 'wsw').find_all('p')])]), \n",
    "                                  'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(svoboda_2020).to_csv('svoboda_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(svoboda_2020).to_csv('svoboda_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(svoboda_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vesti FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "times = [['000000', '120000'], ['120000', '235959']]\n",
    "radiovesti_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        radiovesti_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        for time in times:\n",
    "            try:\n",
    "                url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.radiovesti.ru%20OR%20domainis:radiovesti.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                radiovesti_20[f'2020.{month:02d}.{day+1:02d}'].extend([a['href'] for a in soup.body.table.find_all('a')])\n",
    "                print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))\n",
    "            except (AttributeError, IndexError):\n",
    "                print(date)\n",
    "with open('radiovesti_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(radiovesti_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiovesti_2020 = []\n",
    "for date in tqdm_notebook(list(radiovesti_20.keys())):\n",
    "    for link in tqdm_notebook(radiovesti_20[date]):\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            if r.status_code==200:\n",
    "                r.encoding = 'utf-8'\n",
    "                soup = BS(r.text)\n",
    "                radiovesti_2020.append({'Media':'radiovesti.ru', 'Date':date, 'Link':link,\n",
    "                                      'Text':' '.join([soup.h1.text, ' '.join([p.text for p in soup.find('div', 'insides-page__news__text').find_all('p')])]), \n",
    "                                      'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(radiovesti_2020).to_csv('radiovesti_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(radiovesti_2020).to_csv('radiovesti_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(radiovesti_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
