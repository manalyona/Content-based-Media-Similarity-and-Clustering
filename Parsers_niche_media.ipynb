{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from math import ceil\n",
    "from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from datetime import date, datetime\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autonews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://www.autonews.ru/ajax/tags/?tag=%D0%9D%D0%BE%D0%B2%D0%BE%D1%81%D1%82%D0%B8&offsetrbc=0&offset=500&limit=500'\n",
    "r = requests.get(link)\n",
    "soup = BS(r.json()['html'])\n",
    "\n",
    "dates = [(div.find('span', 'item-medium__date').text, div.find('a', 'item-medium__link')['href']) for div in soup.find_all('div', 'item-medium')]\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autonews_20 = defaultdict(list)\n",
    "for i in dates[1:]:\n",
    "    if 'января' in i[0]:\n",
    "        date = '2020.01.'+i[0].split()[0]\n",
    "    if 'февраля' in i[0]:\n",
    "        date = '2020.02.'+i[0].split()[0]\n",
    "    autonews_20[date].append(i[1])\n",
    "autonews_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autonews_2020 = []\n",
    "for date in tqdm_notebook(sorted(list(autonews_20.keys()))):\n",
    "    for link in tqdm_notebook(autonews_20[date]):\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            soup = BS(r.text)\n",
    "            autonews_2020.append({'Media':'autonews.ru', 'Date':date, 'Link':link,\n",
    "                                  'Text':' '.join([soup.find('div', 'article__header__title').text, ' '.join([p.text for p in soup.find('div', 'article__text').find_all('p') if p.text]).replace('\\xa0', ' ')]), \n",
    "                                  'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(autonews_2020).to_csv('autonews_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(autonews_2020).to_csv('autonews_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(autonews_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Za Rulyem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "times = [['000000', '120000'], ['120000', '235959']]\n",
    "zarulem_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        zarulem_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        for time in times:\n",
    "            try:\n",
    "                url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.zr.ru%20OR%20domainis:zr.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                zarulem_20[f'2020.{month:02d}.{day+1:02d}'].extend([a['href'] for a in soup.body.table.find_all('a')])\n",
    "                print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))\n",
    "            except (AttributeError, IndexError):\n",
    "                print(date)\n",
    "                \n",
    "with open('zarulem_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(zarulem_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarulem_2020 = []\n",
    "for date in tqdm_notebook(list(zarulem_20.keys())):\n",
    "    for link in tqdm_notebook(zarulem_20[date]):\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            soup = BS(r.text)\n",
    "            zarulem_2020.append({'Media':'zr.ru', 'Date':date, 'Link':link,\n",
    "                                  'Text':' '.join([soup.h1.text, ' '.join([p.text.strip() for p in soup.find('div', 'stroy_content').find_all('p') if p.text.strip() and 'Фото' not in p.text])]).replace('\\xa0', ' '),\n",
    "                                 'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(zarulem_2020).to_csv('zarulem_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(zarulem_2020).to_csv('zarulem_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(zarulem_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoreview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "times = [['000000', '120000'], ['120000', '235959']]\n",
    "review_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        review_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        for time in times:\n",
    "            try:\n",
    "                url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.autoreview.ru%20OR%20domainis:autoreview.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                review_20[f'2020.{month:02d}.{day+1:02d}'].extend([a['href'] for a in soup.body.table.find_all('a')])\n",
    "                print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))\n",
    "            except (AttributeError, IndexError):\n",
    "                print(date)\n",
    "                \n",
    "with open('review_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(review_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_2020 = []\n",
    "for date in tqdm_notebook(list(review_20.keys())):\n",
    "    for link in tqdm_notebook(review_20[date]):\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            soup = BS(r.text)\n",
    "            review_2020.append({'Media':'autoreview.ru', 'Date':date, 'Link':link,\n",
    "                                  'Text':' '.join([soup.h1.text, ' '.join([p.text for p in soup.find('div', 'full-article').find_all('p')])]).replace('\\xa0', ' '),\n",
    "                                 'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(review_2020).to_csv('review_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(review_2020).to_csv('review_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(review_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_motor(link):\n",
    "    r = requests.get(link)\n",
    "    soup = BS(r.text)\n",
    "    \n",
    "    try:\n",
    "        title = soup.h1.div.text\n",
    "        lead = soup.find('div', 'lead').div.text\n",
    "        text = ' '.join([p.text for div in soup.find_all('div', 'text') for p in div.find_all('p')])\n",
    "        return ' .'.join([title, lead, text])\n",
    "    except (AttributeError, IndexError):\n",
    "        title = soup.h1.div.text\n",
    "        text = ' '.join([p.text for div in soup.find_all('div', 'text') for p in div.find_all('p')])\n",
    "        return ' .'.join([title, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "motor_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020/{month:02d}/{day+1:02d}'\n",
    "        try:\n",
    "            url = f'https://motor.ru/pulse/{date}'\n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.text)\n",
    "            links = ['https://motor.ru'+a['href'] for a in soup.find_all('div', 'block')[1].find_all('a') \n",
    "                     if 'authors' not in a['href'] and 'pulse' not in a['href']]\n",
    "            motor_20[f'2020.{month:02d}.{day+1:02d}'] = links\n",
    "            print(f'2020.{month:02d}.{day+1:02d}', len(links))\n",
    "        except (AttributeError, IndexError):\n",
    "            print(date)\n",
    "with open('motor_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(motor_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_2020 = []\n",
    "for date in tqdm_notebook(list(motor_20.keys())):\n",
    "    for link in tqdm_notebook(motor_20[date]):\n",
    "        try:\n",
    "            motor_2020.append({'Media':'motor.ru', 'Date':date, 'Link':link,\n",
    "                                       'Text':get_article_motor(link), 'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(motor_2020).to_csv('motor_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(motor_2020).to_csv('motor_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(motor_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sport Express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = defaultdict(list)\n",
    "for page in tqdm_notebook(range(282, 489)):\n",
    "    link = f'https://www.sport-express.ru/search/page{page}/?sw=%F7%F2%EE&sportType=%C2%E8%E4+%F1%EF%EE%F0%F2%E0&kolonki=%CA%EE%EB%EE%ED%EA%E8&dt_to=&tp%5B0%5D=news'\n",
    "    r = requests.get(link)\n",
    "    soup = BS(r.text)\n",
    "    for item in [(div.find('a', 'se19-search-item__title')['href'], \n",
    "                  div.find('p', 'se19-search-item__date').text.split(',')[0])\n",
    "                 for div in soup.find('div', 'se19-search__results').find_all('div', 'se19-search-item')]:\n",
    "        if 'января' in item[1]:\n",
    "            dates[f'{item[1].split()[2]}.01.{item[1].split()[0]}'].append(item[0])\n",
    "        if 'февраля' in item[1]:\n",
    "            dates[f'{item[1].split()[2]}.02.{item[1].split()[0]}'].append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "express_2020 = []\n",
    "for date in tqdm_notebook(sorted(list(dates.keys()))):\n",
    "    if len(date) == 9:\n",
    "        date = date[:-1]+'0'+date[-1]\n",
    "    for link in tqdm_notebook(dates[date]):\n",
    "#         if link not in pd.DataFrame(echo_2020).Link:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            soup = BS(r.text)\n",
    "            express_2020.append({'Media':'sport-express.ru', 'Date':date, 'Link':link,\n",
    "                                  'Text':' '.join([soup.h1.text, ' '.join([p.text for p in soup.find('div', 'article_text').find_all('p')])]).replace('\\xa0', ' '),\n",
    "                                 'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(express_2020).to_csv('express_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(express_2020).to_csv('express_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(express_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Championat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "times = [['000000', '120000'], ['120000', '235959']]\n",
    "championat_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        championat_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        for time in times:\n",
    "            try:\n",
    "                url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.championat.com%20OR%20domainis:championat.com)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                championat_20[f'2020.{month:02d}.{day+1:02d}'].extend([a['href'] for a in soup.body.table.find_all('a')])\n",
    "                print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))\n",
    "            except (AttributeError, IndexError):\n",
    "                print(date)\n",
    "with open('championat_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(championat_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "championat_2020 = []\n",
    "for date in tqdm_notebook(list(championat_20.keys())):\n",
    "    for link in tqdm_notebook(championat_20[date]):\n",
    "        try:\n",
    "            r = requests.get(link, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:75.0) Gecko/20100101 Firefox/75.0'})\n",
    "            soup = BeautifulSoup(r.text)\n",
    "            championat_2020.append({'Media':'championat.ru', 'Date':date, 'Link':link,\n",
    "                                  'Text':' '.join([soup.find('div', 'article-head__title').text, \n",
    "                                                   ' '.join([p.text for p in \n",
    "                                                             soup.find('div', 'article-content').find_all('p')])]), \n",
    "                                  'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(championat_2020).to_csv('championat_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(championat_2020).to_csv('championat_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(championat_2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sports.Ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_sports(link):\n",
    "    r = requests.get(link)\n",
    "    soup = BS(r.text)\n",
    "\n",
    "    title = soup.h1.text.strip()\n",
    "    text = ' '.join([p.text for p in soup.find('div', 'news-item__content').find_all('p') if 'Facebook' not in p.text][:-1])\n",
    "    return ' .'.join([title, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "sports_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020/{month}/{day+1:02d}'\n",
    "#         sports_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        try:\n",
    "            url = f'https://www.sports.ru/stat/news/{date}.html'\n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.text)\n",
    "            links = ['https://www.sports.ru'+li.a['href'] for li in \n",
    "                     soup.find('ul', 'archive-list').find_all('li') if 'https:' not in li.a['href']]\n",
    "            sports_20[f'2020.{month:02d}.{day+1:02d}'] = links\n",
    "            print(f'2020.{month:02d}.{day+1:02d}', len(links))\n",
    "        except (AttributeError, IndexError):\n",
    "            print(date)\n",
    "with open('sports_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(sports_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_2020 = []\n",
    "for date in tqdm_notebook(list(sports_20.keys())):\n",
    "    for link in tqdm_notebook(sports_20[date]):\n",
    "        try:\n",
    "            sports_2020.append({'Media':'sports.ru', 'Date':date, 'Link':link,\n",
    "                                       'Text':get_article_sports(link), 'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(sports_2020).to_csv('sports_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(sports_2020).to_csv('sports_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(sports_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['%D0%BE%D0%B1%D0%BD%D0%B0%D1%80%D1%83%D0%B6%D0%B8%D0%BB%D0%B8', # обнаружили\n",
    "        '%D0%B8%D1%81%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5', # исследование\n",
    "        '%D1%81%D1%82%D0%B0%D1%82%D1%8C%D1%8F', # статья\n",
    "         '%D0%B6%D1%83%D1%80%D0%BD%D0%B0%D0%BB%D0%B5', # журнале\n",
    "         '%D1%83%D1%87%D0%B5%D0%BD%D1%8B%D0%B5'] # ученые\n",
    "         '%D0%BD%D0%B0%D1%88%D0%BB%D0%B8', # нашли\n",
    "        ]\n",
    "links = set()\n",
    "for text in texts:\n",
    "    link = f'https://nplus1.ru/search?q={text}'\n",
    "    r = requests.get(link)\n",
    "    soup = BS(r.text)\n",
    "    links.update(['https://nplus1.ru'+a['href'] for a in soup.find('ol', 'list-search-results').find_all('a')\n",
    " if '2019' in a['href'] or '2020/01' in a['href'] or '2020/02' in a['href']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplus1_19_20 = []\n",
    "for link in tqdm_notebook(links):\n",
    "    if '2020' in link:\n",
    "        date = link[link.index('/2020/')+1:link.index('/2020/')+11].replace('/', '.')\n",
    "    if '2019' in link:\n",
    "        date = link[link.index('/2019/')+1:link.index('/2019/')+11].replace('/', '.')    \n",
    "    try:\n",
    "        r = requests.get(link)\n",
    "        soup = BS(r.text)\n",
    "        nplus1_19_20.append({'Media':'nplus1.ru', 'Date':date, 'Link':link,\n",
    "                              'Text':'. '.join([soup.h1.text, ' '.join([p.text for p in soup.find('div', 'body js-mediator-article').find_all('p') if p.text and 'class' not in p.attrs]).replace('\\xa0', ' ')]), \n",
    "                              'Dep':'', 'Tags':[]})\n",
    "    except (requests.exceptions.ConnectionError, AttributeError, \n",
    "            IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "        print(link)\n",
    "if date.endswith('1'):\n",
    "    pd.DataFrame(nplus1_19_20).to_csv('nplus1_19_20.csv', encoding='utf-8')\n",
    "pd.DataFrame(nplus1_19_20).to_csv('nplus1_19_20.csv', encoding='utf-8')\n",
    "pd.DataFrame(nplus1_19_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplus1_19_20 = pd.DataFrame(nplus1_19_20).sort_values(by='Date').reset_index().drop(['index'], axis = 1)\n",
    "nplus1_19_20.to_csv('nplus1_19_20_order.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postnauka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_names = {'Января':'01', 'Февраля':'02', 'Марта':'03', 'Апреля':'04', \n",
    "                'Мая':'05', 'Июня':'06', 'Июля':'07', 'Августа':'08', \n",
    "                'Сентября':'09', 'Октября':'10', 'Ноября':'11', 'Декабря':'12'}\n",
    "postnauka_19_20 = []\n",
    "for page in tqdm_notebook(range(151)):\n",
    "    url = f'https://postnauka.ru/api/v4/grid?page={page+1}'\n",
    "    r = requests.get(url)\n",
    "    for link in tqdm_notebook(['https://postnauka.ru'+i['url'] for i in r.json() if 'courses' not in i['url']]):\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            soup = BS(r.text)\n",
    "            date = soup.find('div', 'pub-data-item').text\n",
    "            if '2019' in date or ('2020' in date and \n",
    "                              (months_names[date.split()[1]]=='01' or months_names[date.split()[1]]=='02')):\n",
    "                postnauka_19_20.append({'Media':'postnauka.ru', 'Date':f'{date.split()[2]}.{months_names[date.split()[1]]}.{date.split()[0]}', 'Link':link,\n",
    "                                  'Text':' '.join([soup.h1.text, ' '.join([p.text for p in soup.find('div', 'article-text').find_all('p')])]).replace('\\xa0', ' ').replace('\\n', ' '), \n",
    "                                  'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(postnauka_19_20).to_csv('postnauka_19_20.csv', encoding='utf-8')\n",
    "pd.DataFrame(postnauka_19_20).to_csv('postnauka_19_20.csv', encoding='utf-8')\n",
    "pd.DataFrame(postnauka_19_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator.Ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = defaultdict(list)\n",
    "rubrics = ['astronomy', 'biology', 'humanitarian-science', 'mathematics', 'earth-science',\n",
    "          'agriculture', 'medicine', 'engineering-science', 'physics', 'chemistry-and-materials']\n",
    "for rubric in tqdm_notebook(rubrics):\n",
    "    link = f'https://indicator.ru/api/v2/topics?include=image&limit=100&offset=0&rubric_root={rubric}&topic_type=-daily_photo%2Cevent&with_filtered_count=1'\n",
    "    r = requests.get(link)\n",
    "    print(rubric, r.json()['meta']['filtered_count'], '\\n')\n",
    "    num_of_pages = ceil(r.json()['meta']['filtered_count']/100)\n",
    "    for i in tqdm_notebook(range(num_of_pages)):\n",
    "        link = f'https://indicator.ru/api/v2/topics?include=image&limit=100&offset={i*100}&rubric_root={rubric}&topic_type=-daily_photo%2Cevent&with_filtered_count=1'\n",
    "        r = requests.get(link)\n",
    "        for item in r.json()['data']:\n",
    "            date = item['attributes']['published_at'][:10]\n",
    "            if '2019' in date or '2020-01' in date or '2020-02' in date:\n",
    "#                 print(date.replace('-', '.'), )\n",
    "                links[date.replace('-', '.')].append('https://indicator.ru'+item['attributes']['link'])\n",
    "    print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_indicator_19_20 = defaultdict(list)\n",
    "links_without_date = []\n",
    "for link in links:\n",
    "    if link.endswith('2019.htm') or link.endswith('01-2020.htm') or link.endswith('02-2020.htm'):\n",
    "        date = link[-14:-4].split('-')\n",
    "        date = f'{date[2]}.{date[1]}.{date[0]}'\n",
    "        links_indicator_19_20[date].append(link)\n",
    "    if not (link.endswith('2020.htm') or link.endswith('2019.htm') or link.endswith('2018.htm')\n",
    "           or link.endswith('2017.htm') or link.endswith('2016.htm')):\n",
    "        links_without_date.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_19_20 = []\n",
    "for date in tqdm_notebook(sorted(list(links.keys()))):\n",
    "    for link in tqdm_notebook(links[date]):\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            soup = BS(r.text)\n",
    "            indicator_19_20.append({'Media':'indicator.ru', 'Date':date, 'Link':link,\n",
    "                                  'Text':' .'.join([soup.h1.text, ' '.join([p.text for p in soup.find('div', 'text').find_all('p') \n",
    "                                   if 'Мои источники' not in p.text and 'Подписывайтесь на Indicator.Ru в соцсетях' not in p.text])]), \n",
    "                                  'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(indicator_19_20).to_csv('indicator_19_20.csv', encoding='utf-8')\n",
    "pd.DataFrame(indicator_19_20).to_csv('indicator_19_20.csv', encoding='utf-8')\n",
    "pd.DataFrame(indicator_19_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
