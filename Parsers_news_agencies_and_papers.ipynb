{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from math import ceil\n",
    "from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from datetime import date, datetime\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "times = [['000000', '060000'], ['060000', '120000'], ['120000', '180000'], ['180000', '235959']]\n",
    "dates = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        dates[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        for time in times:\n",
    "            url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.tass.ru%20OR%20domainis:tass.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.text)\n",
    "            dates[f'2020.{month:02d}.{day+1:02d}'].extend([a['href'] for a in soup.body.table.find_all('a')])\n",
    "            print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tass_2020 = pd.DataFrame(columns = ['Media', 'Date', 'Link', 'Text', 'Dep', 'Tags'])\n",
    "\n",
    "for date in tqdm_notebook(list(dates.keys())):\n",
    "    for link in tqdm_notebook(dates[date]):\n",
    "        if link not in tass_2020.Link.unique():\n",
    "            try:\n",
    "                tass_2020 = tass_2020.append({'Media':'tass.ru', 'Date':date, 'Link':link,\n",
    "                                           'Text':get_article_tass(link)}, ignore_index=True)\n",
    "            except (AttributeError, IndexError):\n",
    "                print(link)\n",
    "    if date.endswith('1'):\n",
    "        tass_2020.to_csv('tass_2020.csv', encoding='utf-8')\n",
    "tass_2020.to_csv('tass_2020.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_ria(link):\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    try:\n",
    "        heading = soup.h1.text\n",
    "        text = ' '.join([i.text for i in soup\n",
    "                  .find('div', 'article__body js-mediator-article mia-analytics')\n",
    "                  .find_all('div', 'article__text')])\n",
    "    except AttributeError:\n",
    "        print('Error ', link)\n",
    "        return\n",
    "    return '. '.join([heading, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "dates = dict()\n",
    "for month in list(months.keys())[:2]:\n",
    "    for day in range(months[month]):\n",
    "        link = f'https://ria.ru/2020{month:0>2}{day+1:0>2}/'\n",
    "        dates[f'2020.{month:0>2}.{day+1:0>2}'] = link\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_ria_2020 = dict()\n",
    "for date in tqdm_notebook(list(dates.keys())):\n",
    "    \n",
    "    browser = webdriver.Chrome('K:\\\\Programs\\\\chromedriver_win32\\\\chromedriver.exe')\n",
    "#     browser.set_window_size(1280, 1024)\n",
    "\n",
    "    browser.get(dates[date])\n",
    "\n",
    "    more_results = browser.find_element_by_class_name('list-more')\n",
    "    more_results.click()\n",
    "\n",
    "    element = browser.find_element_by_tag_name('body')\n",
    "    for i in range(200):\n",
    "        element.send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    page_source = browser.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "    links = [a['href'] for a in soup.find_all('a', 'list-item__title')]\n",
    "    links_ria_2020[date] = links\n",
    "\n",
    "    print(f'{date}: {len(links)}')\n",
    "\n",
    "    browser.close()\n",
    "    \n",
    "with open('ria_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(links_ria_2020, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ria_2020 = pd.DataFrame(columns = ['Media', 'Date', 'Link', 'Text', 'Dep', 'Tags'])\n",
    "\n",
    "for date in tqdm_notebook(list(links_ria_2020.keys())):\n",
    "    for link in tqdm_notebook(links_ria_2020[date]):\n",
    "        if link not in ria_2020.Link:\n",
    "            try:\n",
    "                ria_2020 = ria_2020.append({'Media':'ria.ru', 'Date':date, 'Link':link, \n",
    "                            'Text':get_article_ria(link)}, ignore_index=True)\n",
    "            except (AttributeError, IndexError):\n",
    "                print(link)\n",
    "    ria_2020.to_csv('ria_2020.csv', encoding = 'utf-8')\n",
    "    \n",
    "ria_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_interfax(date): #format: '26.11.2019'\n",
    "\n",
    "    page = 1\n",
    "    months = {'01':'января', '02':'февраля', '03':'марта', '04':'апреля', \n",
    "              '05':'мая', '06':'июня', '07':'июля', '08':'августа', \n",
    "              '09':'сентября', '10':'октября', '11':'ноября', '12':'декабря'}\n",
    "    code = dict(zip('абвгдеийклмнопрустфьюя', \n",
    "                    ['%E0', '%E1', '%E2', '%E3', '%E4', '%E5', \n",
    "                     '%E8', '%E9', '%EA', '%EB', '%EC', '%ED', \n",
    "                     '%EE', '%EF', '%F0', '%F3', '%F1', '%F2', \n",
    "                     '%F4', '%FC', '%FE', '%FF']))\n",
    "    sw = ''.join([code[letter] for letter in months[date.split('.')[1]]])\n",
    "\n",
    "    link = f'https://www.interfax.ru/search/?sw={sw}&df={date}&dt={date}&sec=0&p=page_{page}'\n",
    "\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "\n",
    "    num_of_articles = int(soup.find('div', 'sPageResult__total').text.split()[1])\n",
    "    num_of_pages = ceil((num_of_articles)/40)\n",
    "\n",
    "    links = set()\n",
    "\n",
    "    for page in range(num_of_pages):\n",
    "\n",
    "        link = f'https://www.interfax.ru/search/?sw={sw}&df={date}&dt={date}&sec=0&p=page_{page+1}'\n",
    "        r = requests.get(link)\n",
    "        soup = BeautifulSoup(r.text)\n",
    "\n",
    "        for i in soup.find('div', 'leftside').find_all('a'):\n",
    "            try:\n",
    "                link = i['href']\n",
    "                if (not link.endswith('/') and \n",
    "                    not link.endswith('=0') and \n",
    "                    'story' not in link and \n",
    "                    'sw' not in link and \n",
    "                    'photo' not in link):\n",
    "\n",
    "                    if 'http' not in link:\n",
    "                        links.add('https://www.interfax.ru'+link)\n",
    "                    else:\n",
    "                        links.add(link)\n",
    "            except KeyError:\n",
    "                if i.attrs['class'] == ['active']:\n",
    "                    pass\n",
    "                else:\n",
    "                    print(i)\n",
    "\n",
    "    return num_of_articles, len(links), links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_interfax(link):\n",
    "    result = ''\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    try:\n",
    "        heading = soup.article.h1.text.encode(encoding='cp1252').decode('cp1251')\n",
    "        result += heading\n",
    "    except UnicodeEncodeError :\n",
    "        print(link, soup.article.h1.text)\n",
    "    try:\n",
    "        text = ' '.join([p.text.encode(encoding='cp1252').decode('cp1251') \n",
    "                     for p in soup.article.find_all('p') if p.text])\n",
    "        result = result + '. ' + text\n",
    "    except UnicodeEncodeError:\n",
    "        print(link, soup.article.find_all('p'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kommersant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_kommers(date): #date in format '26.11.2019'\n",
    "    \n",
    "    #get first page of results and find the number of such pages (each contains 10 articles)\n",
    "    search_query='%D0%B8' # letter \"и\", can't be empty\n",
    "    page='1'\n",
    "    date=str(date)\n",
    "    link=f'https://www.kommersant.ru/search/results?places=&categories=&isbankrupt=\\\n",
    "    &datestart={date}&dateend={date}&sort_type=1&sort_dir=&regions=&results_count=\\\n",
    "    &page={page}&search_query={search_query}&sort_by=1&search_full=1\\\n",
    "    &time_range=2&dateStart={date}&dateEnd={date}'\n",
    "\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "\n",
    "    num_of_articles=(soup.body.find_all('div', {'class':'layout'})[4]\n",
    "                     .find('form', {'id':'frmSearch'})\n",
    "                     .find('h3').text.split()[4])\n",
    "\n",
    "    #gather links from the pages\n",
    "    results_day=[]\n",
    "    for page in range(ceil(int(num_of_articles)/10)):\n",
    "\n",
    "        link=f'https://www.kommersant.ru/search/results?places=&categories=&isbankrupt=\\\n",
    "        &datestart={date}&dateend={date}&sort_type=1&sort_dir=&regions=&results_count=\\\n",
    "        &page={page+1}&search_query={search_query}&sort_by=1&search_full=1\\\n",
    "        &time_range=2&dateStart={date}&dateEnd={date}'\n",
    "\n",
    "        r = requests.get(link)\n",
    "        soup = BeautifulSoup(r.text)\n",
    "\n",
    "        try:\n",
    "            results=(soup.body\n",
    "                 .find_all('div', {'class':'layout'})[4]\n",
    "                 .find('div', {'class':'grid grid--main'})\n",
    "                 .find('div', {'class':'grid_cell grid_cell_large'})\n",
    "                 .find('div', {'class':'search_results_holder'})\n",
    "                 .find_all('div', {'class':'search_results_item'}))\n",
    "        except AttributeError:\n",
    "            print(page)\n",
    "\n",
    "        for i in results:\n",
    "            results_day.append('https://www.kommersant.ru'+i.find('h2').find('a')['href'].split('?')[0])\n",
    "\n",
    "    return results_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_kommers(link): #like this: 'https://www.kommersant.ru/doc/4149269'\n",
    "\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "\n",
    "    if r.status_code!=404:\n",
    "        try:\n",
    "            article = soup.body.find_all('div', {'class':'layout'})[4]\n",
    "    #      .find('div', {'class':'grid grid--main'})\n",
    "    #      .find('div', {'class':'grid_cell grid_cell_big js-middle'})\n",
    "    #      .find('div', {'class':'lenta js-lenta'})\n",
    "    #      .find('article', {'class':'b-article'}))\n",
    "            try:\n",
    "                header = ' '.join([article.header.find('div', 'text').h2.text, \n",
    "                                  article.header.find('div', 'text').h1.text])\n",
    "            except AttributeError: #some articles doesn't have subtitles\n",
    "                header = article.header.find('div', 'text').h1.text\n",
    "\n",
    "            text = ' '.join([p.text for p in article.find('div', 'article_text_wrapper').find_all('p')])\n",
    "\n",
    "            return ' '.join([header, text])\n",
    "    \n",
    "        except AttributeError as err:\n",
    "            print(\"Something's wrong with\", link, '\\n', err)\n",
    "#             article = soup.body.find_all('div', {'class':'layout'})[4]\n",
    "    #         return link\n",
    "        except IndexError as ind:\n",
    "            print(\"Something's wrong with\", link, '\\n', ind)\n",
    "    \n",
    "    else: \n",
    "        print(r, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_commers_2019 = dict()\n",
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "years = [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "for month in tqdm_notebook(list(months.keys())):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        links = get_links_kommers(f'{day+1:0>2}.{month:0>2}.2019')\n",
    "        links_commers_2019[f'2019.{month:0>2}.{day+1:0>2}'] = links\n",
    "    with open('links_commers_2019.json', 'w', encoding = 'utf-8') as f:\n",
    "        json.dump(links_commers_2019, f)\n",
    "links_commers_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commers_2019 = pd.DataFrame(columns = ['Media', 'Date', 'Link', 'Text', 'Dep', 'Tags'])\n",
    "for date in tqdm_notebook(list(links_commers_2019.keys())):\n",
    "    for link in links_commers_2019[date]:\n",
    "        commers_2019 = commers_2019.append({'Media':'kommersant.ru', 'Date':date, 'Link':link,\n",
    "                                           'Text':get_article_kommers(link)}, ignore_index=True)\n",
    "    if date.endswith('1'):\n",
    "        commers_2019.to_csv('commers_2019.csv', encoding='utf-8')\n",
    "commers_2019.to_csv('commers_2019.csv', encoding='utf-8')\n",
    "commers_2019 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vedomosti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "dates = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        r = requests.get(f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.vedomosti.ru%20OR%20domainis:vedomosti.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}000000&enddatetime={date}235959')\n",
    "        soup = BeautifulSoup(r.text)\n",
    "        dates[f'2020.{month:02d}.{day+1:02d}'] = [a['href'] for a in soup.body.table.find_all('a')]\n",
    "        print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vedomosti_2020 = pd.DataFrame(columns = ['Media', 'Date', 'Link', 'Text', 'Dep', 'Tags'])\n",
    "\n",
    "for date in tqdm_notebook(links_2020.keys()):\n",
    "    for link in tqdm_notebook(links_2020[date]):\n",
    "        if 'vedomosti' in link:\n",
    "            try:\n",
    "                r = requests.get(link)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                heading = soup.h1.text.strip()\n",
    "                text = ' '.join([p.text for p in soup.find(\"div\", 'article__boxes').\n",
    "                                 find_all('p', 'box-paragraph__text')])\n",
    "                vedomosti_2020 = vedomosti_2020.append({'Media':'vedomosti.ru', 'Date':date, 'Link':link,\n",
    "                                           'Text':'. '.join([heading, text])}, ignore_index=True)\n",
    "            except (AttributeError, IndexError):\n",
    "                print(link)\n",
    "    if date.endswith('1'):\n",
    "        vedomosti_2020.to_csv('vedomosti_2020.csv', encoding='utf-8')\n",
    "vedomosti_2020.to_csv('vedomosti_2020.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rossiyskaya Gazeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_rosgazeta(date): # like this: '01.03.2019'\n",
    "\n",
    "    browser = webdriver.Chrome('C:\\\\Users\\\\manal\\\\chromedriver')\n",
    "    browser.set_window_size(1080, 1024)\n",
    "\n",
    "    browser.get(f'https://rg.ru/search/?from={date}&to={date}&?keywords=и')\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    num_of_pages = int(browser.find_element_by_class_name('b-search-info__meta').text.split()[0])\n",
    "\n",
    "    body = browser.find_element_by_tag_name('body')\n",
    "\n",
    "    for i in range(num_of_pages//15+1):\n",
    "        for j in range(6):\n",
    "            body.send_keys(Keys.PAGE_DOWN)\n",
    "        try:\n",
    "            browser.find_element_by_class_name('b-link-btn').click()\n",
    "        except selenium.common.exceptions.ElementClickInterceptedException:\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "\n",
    "    page_source = browser.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    \n",
    "    browser.close()\n",
    "\n",
    "    return ['https://rg.ru'+h2.a['href'] for h2 in soup.find_all('h2', 'b-news-inner__list-item-title')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_rosgazeta(link):\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    try:\n",
    "        heading = soup.h1.text.strip()\n",
    "        text = ' '.join([p.text for p in soup.find('div', 'b-material-wrapper b-material-wrapper_art').find_all('p')])\n",
    "        return '. '.join([heading, text])\n",
    "    except AttributeError:\n",
    "        try: \n",
    "            heading = soup.h1.text.strip()\n",
    "            text = ' '.join([p.text for p in soup.find('div', 'b-material-wrapper__text').find_all('p')])\n",
    "            return '. '.join([heading, text])\n",
    "        except AttributeError:    \n",
    "            print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_rosgazeta_2020 = dict()\n",
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "years = [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "for month in tqdm_notebook(list(months.keys())[1:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))[16:]):\n",
    "        try:\n",
    "            links = get_links_rosgazeta(f'{day+1:0>2}.{month:0>2}.2020')\n",
    "            links_rosgazeta_2020[f'2020.{month:0>2}.{day+1:0>2}'] = links\n",
    "        except selenium.common.exceptions.NoSuchElementException:\n",
    "            print(f'{day+1:0>2}.{month:0>2}.2020')\n",
    "    with open('rosgazeta_links_2020_02.json', 'w', encoding = 'utf-8') as f:\n",
    "        json.dump(links_rosgazeta_2020, f)\n",
    "links_rosgazeta_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rosgazeta_2020 = pd.DataFrame(columns = ['Media', 'Date', 'Link', 'Text', 'Dep', 'Tags'])\n",
    "for date in tqdm_notebook(list(links_rosgazeta_2020.keys())[-6:]):\n",
    "    for link in links_rosgazeta_2020[date]:\n",
    "        if ('prikaz' not in link) and ('-dok' not in link) and (link not in rosgazeta_2020.Link.values):\n",
    "            rosgazeta_2020 = rosgazeta_2020.append({'Media':'rg.ru', 'Date':date, 'Link':link,\n",
    "                                           'Text':get_article_rosgazeta(link)}, ignore_index=True)\n",
    "    if date.endswith('1'):\n",
    "        rosgazeta_2020.to_csv('rosgazeta_2020.csv', encoding='utf-8')\n",
    "rosgazeta_2020.to_csv('rosgazeta_2020.csv', encoding='utf-8')\n",
    "rosgazeta_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novaya Gazeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_novaya(date): #like this: '23.07.19'\n",
    "    \n",
    "    months_names = {'01':'января', '02':'февраля', '03':'марта', '04':'апреля', \n",
    "                  '05':'мая', '06':'июня', '07':'июля', '08':'августа', \n",
    "                  '09':'сентября', '10':'октября', '11':'ноября', '12':'декабря'}\n",
    "    q = months_names[date.split('.')[1]]\n",
    "    \n",
    "    link = f'https://content.novayagazeta.ru/search?q={q}&date_from={date}&date_to={date}&limit=100'\n",
    "\n",
    "    r = requests.get(link)\n",
    "    \n",
    "    links = []\n",
    "    for item in r.json()['items']:\n",
    "        if item['type'] == 'news_entry':\n",
    "            links.append('https://novayagazeta.ru/news/'+item['code'])\n",
    "        elif item['type'] == 'article' or item['type'] == 'photo' or item['type'] == 'video':\n",
    "            links.append('https://novayagazeta.ru/articles/'+item['code'])\n",
    "        else:\n",
    "            print(item['code'])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_novaya(link):\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    try:\n",
    "        heading = soup.h1.text\n",
    "        text = ' '.join([p.text for p in soup.find_all('p') if len(p.attrs)==0])\n",
    "        return '. '.join([heading, text.replace('\\xa0', ' ')])\n",
    "    except AttributeError:\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novaya_links_2020 = dict()\n",
    "\n",
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(range(months[month])):\n",
    "        try:\n",
    "            links = get_links_novaya(f'{day+1:0>2}.{month:0>2}.20')\n",
    "            novaya_links_2020[f'2020.{month:0>2}.{day+1:0>2}'] = links\n",
    "        except AttributeError:\n",
    "            print(f'{day+1:0>2}.{month:0>2}.20')\n",
    "        with open('novaya_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "            json.dump(novaya_links_2020, f)\n",
    "novaya_links_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novaya_2020 = []\n",
    "for date in tqdm_notebook(list(novaya_links_2020.keys())):\n",
    "    for link in novaya_links_2020[date]:\n",
    "        if ('prikaz' not in link) and ('-dok' not in link):\n",
    "            try:\n",
    "                novaya_2020.append({'Media':'novayagazeta.ru', 'Date':date, 'Link':link,\n",
    "                                           'Text':get_article_novaya(link), 'Dep':'', 'Tags':[]})\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(novaya_2020).to_csv('novaya_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(novaya_2020).to_csv('novaya_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(novaya_2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AiF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_aif(date): #'2020-02-01'\n",
    "    url = f'https://aif.ru/search/index/index/from/{date}/to/{date}/text/%D1%87%D1%82%D0%BE'\n",
    "    links = []\n",
    "    for i in range(20):\n",
    "        json = {'page':i+1}\n",
    "        r = requests.get(url, json)\n",
    "        soup = BeautifulSoup(r.text)\n",
    "        links_temp = [div.find('a')['href'] for div in soup.find_all('div', 'text_box')]\n",
    "        if links_temp:\n",
    "            links.extend(links_temp)\n",
    "        else:\n",
    "#             print(i+1)\n",
    "            break\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_aif(link):\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    try:\n",
    "        heading = soup.h1.text\n",
    "        text = ' '.join([p.text for p in soup.find('div', 'article_text').find_all('p') \n",
    "                         if p.text and 'Подробнее' not in p.text]).replace('\\xa0', ' ')\n",
    "        return '. '.join([heading, text])\n",
    "    except AttributeError:\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aif_links_2020 = dict()\n",
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        try:\n",
    "            links = get_links_aif(f'2020-{month:0>2}-{day+1:0>2}')\n",
    "            aif_links_2020[f'2020.{month:0>2}.{day+1:0>2}'] = links\n",
    "        except IndexError:\n",
    "            print(f'2020-{month:0>2}-{day+1:0>2}')\n",
    "            \n",
    "        if day%5 == 0:\n",
    "            with open('aif_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "                json.dump(aif_links_2020, f)\n",
    "with open('aif_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(aif_links_2020, f)\n",
    "aif_links_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aif_2020 = []\n",
    "for date in tqdm_notebook(list(aif_links_2020.keys())):\n",
    "    for link in aif_links_2020[date]:\n",
    "        if 'gallery' not in link:\n",
    "            aif_2020.append({'Media':'aif.ru', 'Date':date,\n",
    "                                        'Link':link, 'Text':get_article_aif(link)})\n",
    "    if date.endswith('01'):\n",
    "        pd.DataFrame(aif_2020).to_csv('aif_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(aif_2020).to_csv('aif_2020.csv', encoding='utf-8')\n",
    "aif_2020 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Izvestiya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_izvestiya(date): #like this: '2020-01-02'\n",
    "    \n",
    "    links = set()\n",
    "    \n",
    "    page = 1\n",
    "    for text in ['заявил', 'говорит', 'считает', 'известия', \n",
    "                 'рассказал', 'написал', 'сообщил', 'сообщает', \n",
    "                 'объяснил', 'попал', 'вышла', 'передает', 'пришли']:\n",
    "        link = f'https://iz.ru/search?type=0&prd=3&from=0&text={text}&date_from={date}&date_to={date}&sort=0'\n",
    "        r = requests.get(link)\n",
    "        soup = BeautifulSoup(r.text)\n",
    "\n",
    "        num_of_articles = int(soup.find('span', 'Number-of-results__nomber').text)\n",
    "        num_of_pages = ceil(num_of_articles/10)\n",
    "\n",
    "        for page in range(num_of_pages):\n",
    "            link = f'https://iz.ru/search?type=0&prd=3&from={page*10}&text={text}&date_from={date}&date_to={date}&sort=0'\n",
    "            r = requests.get(link)\n",
    "            soup = BeautifulSoup(r.text)\n",
    "            for div in soup.find_all('div', 'view-search'):\n",
    "                links.add(div.a['href'])\n",
    "    return list(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_izvestiya(link):\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "\n",
    "    heading = soup.h1.span.text\n",
    "    text = ' '.join([p.text for p in soup.find('div', 'text-article').find_all('p')])\n",
    "    if not text:\n",
    "        text = soup.find('div', 'text-article__inside').div.text.replace('\\n', ' ')\n",
    "    return '. '.join([heading, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "\n",
    "izvestiya_links_2019 = dict()\n",
    "\n",
    "for month in tqdm_notebook(list(months.keys())[-1:]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            links = get_links_izvestiya(f'2019-{month:0>2}-{day+1:0>2}')\n",
    "            izvestiya_links_2019[f'2019.{month:0>2}.{day+1:0>2}'] = links\n",
    "        except (AttributeError, IndexError, TimeoutError):\n",
    "            print(f'2019-{month:0>2}-{day+1:0>2}')\n",
    "\n",
    "        if day%5 == 0:\n",
    "            with open('izvestiya_links_2019.json', 'w', encoding = 'utf-8') as f:\n",
    "                json.dump(izvestiya_links_2019, f)\n",
    "            \n",
    "with open('izvestiya_links_2019.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(izvestiya_links_2019, f)\n",
    "izvestiya_links_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izvestiya_2019 = []\n",
    "for date in tqdm_notebook(list(izvestiya_links_2019.keys())):\n",
    "    for link in izvestiya_links_2019[date]:\n",
    "        try:\n",
    "            izvestiya_2019.append({'Media':'izvestiya.ru', 'Date':date, 'Link':link,\n",
    "                                       'Text':get_article_izvestiya(link), 'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, IndexError, TimeoutError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(izvestiya_2019).to_csv('izvestiya_2019.csv', encoding='utf-8')\n",
    "pd.DataFrame(izvestiya_2019).to_csv('izvestiya_2019.csv', encoding='utf-8')\n",
    "pd.DataFrame(izvestiya_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Komsomol'skaya Pravda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_kp(link):\n",
    "    try:\n",
    "        soup = BeautifulSoup(requests.get(link).text)\n",
    "        text = ' '.join([soup.h1.text, soup.find('div', 'ArticleDescription').text, ' '.join([p.text for p in soup.find('div', 'text').find_all('p') if p.text and 'ИСТОЧНИК' not in p.text])])\n",
    "        return text\n",
    "    except (AttributeError, IndexError):\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "times = [['000000', '060000'], ['060000', '120000'], ['120000', '180000'], ['180000', '235959']]\n",
    "kom_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        kom_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        for time in times:\n",
    "            url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.kp.ru%20OR%20domainis:kp.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "#             url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.tass.ru%20OR%20domainis:tass.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.text)\n",
    "            kom_20[f'2020.{month:02d}.{day+1:02d}'].extend([a['href'] for a in soup.body.table.find_all('a')])\n",
    "            print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))\n",
    "with open('kp_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(kom_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_2020 = pd.DataFrame(columns = ['Media', 'Date', 'Link', 'Text', 'Dep', 'Tags'])\n",
    "\n",
    "for date in tqdm_notebook(list(kom_20.keys())):\n",
    "    for link in tqdm_notebook(kom_20[date]):\n",
    "        if link not in kp_2020.Link.unique():\n",
    "            try:\n",
    "                kp_2020 = kp_2020.append({'Media':'kp.ru', 'Date':date, 'Link':link,\n",
    "                                           'Text':get_article_kp(link)}, ignore_index=True)\n",
    "            except (AttributeError, IndexError):\n",
    "                print(link)\n",
    "    if date.endswith('1'):\n",
    "        kp_2020.to_csv('kp_2020.csv', encoding='utf-8')\n",
    "kp_2020.to_csv('kp_2020.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moskovskiy Komsomolets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_mosc_comsomol(link):\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    heading = soup.h1.text\n",
    "    text = (' '.join([p.text for p in soup.find('div', 'content').find_all('p')])\n",
    "            .replace('\\xa0', ' ').replace('\\n', ' '))\n",
    "    return ' '.join([heading, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "times = [['000000', '120000'], ['120000', '235959']]\n",
    "mk_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        mk_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        for time in times:\n",
    "            try:\n",
    "                url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.mk.ru%20OR%20domainis:mk.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                mk_20[f'2020.{month:02d}.{day+1:02d}'].extend([a['href'] for a in soup.body.table.find_all('a')])\n",
    "                print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))\n",
    "            except (AttributeError, IndexError):\n",
    "                print(date)\n",
    "with open('mk_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(mk_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosc_comsomol_2020 = []\n",
    "for date in tqdm_notebook(list(mk_20.keys())):\n",
    "    for link in tqdm_notebook(mk_20[date]):\n",
    "        if link not in pd.DataFrame(mosc_comsomol_2020).Link:\n",
    "            try:\n",
    "                mosc_comsomol_2020.append({'Media':'mk.ru', 'Date':date, 'Link':link,\n",
    "                                           'Text':get_article_mosc_comsomol(link), 'Dep':'', 'Tags':[]})\n",
    "            except (requests.exceptions.ConnectionError, AttributeError, IndexError):\n",
    "                print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(mosc_comsomol_2020).to_csv('mosc_comsomol_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(mosc_comsomol_2020).to_csv('mosc_comsomol_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(mosc_comsomol_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
