{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from math import ceil\n",
    "from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from datetime import date, datetime\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_rbc(link):\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    heading = soup.h1.text\n",
    "    try:\n",
    "        lead = soup.find('div', 'article__text article__text_free').span.text\n",
    "    except:\n",
    "        lead = ''\n",
    "    text = ' '.join([p.text for p in soup.find('div', 'article__text article__text_free').find_all('p')])\n",
    "    return '. '.join([heading, lead, text]).replace('\\xa0', ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "times = [['000000', '120000'], ['120000', '235959']]\n",
    "rbc_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        rbc_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        for time in times:\n",
    "            try:\n",
    "                url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.rbc.ru%20OR%20domainis:rbc.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "#                 url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.mk.ru%20OR%20domainis:mk.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "    #             url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.kp.ru%20OR%20domainis:kp.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                rbc_20[f'2020.{month:02d}.{day+1:02d}'].extend([a['href'] for a in soup.body.table.find_all('a')])\n",
    "                print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))\n",
    "            except (AttributeError, IndexError):\n",
    "                print(date)\n",
    "with open('rbc_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(rbc_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbc_2020 = []\n",
    "for date in tqdm_notebook(list(rbc_20.keys())):\n",
    "    for link in tqdm_notebook(rbc_20[date]):\n",
    "        if link not in pd.DataFrame(rbc_2020).Link:\n",
    "            try:\n",
    "                rbc_2020.append({'Media':'rbc.ru', 'Date':date, 'Link':link,\n",
    "                                           'Text':get_article_rbc(link), 'Dep':'', 'Tags':[]})\n",
    "            except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                    IndexError, requests.exceptions.TooManyRedirects):\n",
    "                print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(rbc_2020).to_csv('rbc_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(rbc_2020).to_csv('rbc_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(rbc_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_rt(link):\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    heading = soup.h1.text\n",
    "    subtitle = soup.find('div', 'article__summary article__summary_article-page js-mediator-article').text\n",
    "    text = ' '.join([p.text for p in soup.find('div', 'article__text article__text_article-page js-mediator-article')\n",
    "                     .find_all('p')])\n",
    "    return '. '.join([heading, subtitle, text]).replace('\\xa0', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "times = [['000000', '080000'], ['080000', '160000'],['160000', '235959']]\n",
    "rt_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        rt_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        for time in times:\n",
    "            try:\n",
    "                url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.russian.rt.com%20OR%20domainis:russian.rt.com)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                rt_20[f'2020.{month:02d}.{day+1:02d}'].extend([a['href'] for a in soup.body.table.find_all('a')])\n",
    "                print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))\n",
    "            except (AttributeError, IndexError):\n",
    "                print(date)\n",
    "with open('rt_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(rt_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_2020 = []\n",
    "for date in tqdm_notebook(list(rt_20.keys())):\n",
    "    for link in tqdm_notebook(rt_20[date]):\n",
    "        try:\n",
    "            rt_2020.append({'Media':'russian.rt.com', 'Date':date, 'Link':link,\n",
    "                                       'Text':get_article_rt(link), 'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(rt_2020).to_csv('rt_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(rt_2020).to_csv('rt_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(rt_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gazeta.Ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_gazeta(link):\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    try:\n",
    "        heading = soup.find('div', 'h_1').text\n",
    "        subtitle = soup.find('h1', 'sub_header').text\n",
    "        lead = soup.find('span', 'intro').text\n",
    "        text = ' '.join([p.text for p in soup.find('div', 'article_text').find_all('p')])\n",
    "    except AttributeError:\n",
    "        heading = soup.find('h1', 'article-text-header').text\n",
    "        subtitle, lead = '', ''\n",
    "        text = ' '.join([p.text for p in soup.find('div', 'article-text-body').find_all('p')])\n",
    "    return '. '.join([heading, subtitle, lead, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gazeta_links_2020 = set()\n",
    "times = [['01.01.2020', '10.01.2020'], ['11.01.2020' ,'20.01.2020'], ['21.01.2020', '31.01.2020'],\n",
    "        ['01.02.2020', '10.02.2020'], ['11.02.2020', '20.02.2020'], ['21.02.2020', '29.02.2020']]\n",
    "for time in times:\n",
    "    for text in ['%E7%E0%FF%E2%E8%EB', '%E3%EE%E2%EE%F0%E8%F2', '%E3%E0%E7%E5%F2%E0',\n",
    "            '%F0%E0%F1%F1%EA%E0%E7%E0%EB', '%ED%E0%EF%E8%F1%E0%EB', '%F1%EE%EE%E1%F9%E8%EB',\n",
    "            '%F1%EE%EE%E1%F9%E0%E5%F2', '%EE%E1%FA%FF%F1%ED%E8%EB', '%EF%EE%EF%E0%EB',\n",
    "            '%E2%FB%F8%EB%E0', '%EF%E5%F0%E5%E4%E0%E5%F2', '%EF%F0%E8%F8%EB%E8']:\n",
    "        page = 1\n",
    "        while True:\n",
    "            len_of_links = len(gazeta_links_2020)\n",
    "    #     for text in ['заявил', 'говорит', 'считает', 'газета', \n",
    "    #                      'рассказал', 'написал', 'сообщил', 'сообщает', \n",
    "    #                      'объяснил', 'попал', 'вышла', 'передает', 'пришли']\n",
    "\n",
    "#             time.sleep(1)\n",
    "            url = f'https://www.gazeta.ru/search.shtml?p=page&tmp_p={page}&text={text}&article=&section=&from={time[0]}&to={time[1]}&sort_order=published_desc'\n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.text)\n",
    "            links_temp = ['https://www.gazeta.ru'+h2.a['href'] for h2 in soup.find_all('h2') \n",
    "                              if '2020' in h2.a['href']]\n",
    "            print(time, text, page, len(links_temp))\n",
    "            if links_temp:\n",
    "                gazeta_links_2020.update(set(links_temp))\n",
    "            else:\n",
    "                break\n",
    "            page += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gazeta_links_20 = defaultdict(set)\n",
    "\n",
    "for link in tqdm_notebook(list(gazeta_links_2020)):\n",
    "    if '/2020/' in link:\n",
    "        index = link.index('/2020/')\n",
    "        gazeta_links_20[link[index+1:index+11].replace('/', '.')].add(link)\n",
    "        \n",
    "gazeta_links_20_dict = dict()\n",
    "for k,v in sorted(gazeta_links_20.items()):\n",
    "    gazeta_links_20_dict[k] = list(v)\n",
    "gazeta_links_20_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gazeta_2020 = []\n",
    "for date in tqdm_notebook(list(gazeta_links_20_dict.keys())):\n",
    "    if 'a' not in date:\n",
    "        for link in tqdm_notebook(gazeta_links_20_dict[date]):\n",
    "    #         if link not in pd.DataFrame(gazeta_2020).Link:\n",
    "            try:\n",
    "                gazeta_2020.append({'Media':'gazeta.ru', 'Date':date, 'Link':link,\n",
    "                                           'Text':get_article_gazeta(link), 'Dep':'', 'Tags':[]})\n",
    "            except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                    IndexError, requests.exceptions.TooManyRedirects):\n",
    "                print(link)\n",
    "        if date.endswith('1'):\n",
    "            pd.DataFrame(gazeta_2020).to_csv('gazeta_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(gazeta_2020).to_csv('gazeta_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(gazeta_2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenta.Ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_lenta(link):\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    heading = soup.h1.text\n",
    "    text = ' '.join([p.text for p in soup.find('div', 'js-topic__text').find_all('p')])\n",
    "    return '. '.join([heading, text]).replace('\\xa0', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "times = [['000000', '080000'], ['080000', '160000'],['160000', '235959']]\n",
    "lenta_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        date = f'2020{month:02d}{day+1:02d}'\n",
    "        lenta_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        for time in times:\n",
    "            try:\n",
    "                url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.lenta.ru%20OR%20domainis:lenta.ru)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "#                 url = f'https://api.gdeltproject.org/api/v2/doc/doc?query=%20(domain:.russian.rt.com%20OR%20domainis:russian.rt.com)%20sourcelang:rus&mode=ArtList&maxrecords=250&sort=DateDesc&format=html&startdatetime={date}{time[0]}&enddatetime={date}{time[1]}'\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text)\n",
    "                lenta_20[f'2020.{month:02d}.{day+1:02d}'].extend([a['href'] for a in soup.body.table.find_all('a')])\n",
    "                print(f'2020.{month:02d}.{day+1:02d}', len(soup.body.table.find_all('a')))\n",
    "            except (AttributeError, IndexError):\n",
    "                print(date)\n",
    "with open('lenta_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(lenta_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenta_2020 = []\n",
    "for date in tqdm_notebook(list(lenta_20.keys())):\n",
    "    for link in tqdm_notebook(lenta_20[date]):\n",
    "#         if link not in pd.DataFrame(lenta_2020).Link:\n",
    "        try:\n",
    "            lenta_2020.append({'Media':'lenta.ru', 'Date':date, 'Link':link,\n",
    "                                       'Text':get_article_lenta(link), 'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(lenta_2020).to_csv('lenta_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(lenta_2020).to_csv('lenta_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(lenta_2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fontanka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_fontanka(date): #linke this '2020-05-04'\n",
    "    page = 1\n",
    "    links = []\n",
    "    while True:\n",
    "        link = f'https://www.fontanka.ru/cgi-bin/search.scgi?query=%D1%87%D1%82%D0%BE&rubric=&fdate={date}&tdate={date}&sortt=date&offset={page}'\n",
    "        r = requests.get(link)\n",
    "        soup = BS(r.text)\n",
    "        links_temp = ['https://www.fontanka.ru'+li.find_all('a')[1]['href'] for li \n",
    "                 in soup.find_all('li') if li.find('div') and 'fontanka' not in li.find_all('a')[1]['href']]\n",
    "        links.extend(links_temp)\n",
    "        if links_temp:\n",
    "            page += 1\n",
    "        else:\n",
    "            break\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontanka_links_2020 = dict()\n",
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "\n",
    "for month in tqdm_notebook(list(months.keys())[:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))):\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            links = get_links_fontanka(f'2020-{month:0>2}-{day+1:0>2}')\n",
    "            fontanka_links_2020[f'2020.{month:0>2}.{day+1:0>2}'] = links\n",
    "            print(f'{day+1:0>2}.{month:0>2}.2020', len(links))\n",
    "        except (AttributeError, IndexError):\n",
    "            print(f'{day+1:0>2}.{month:0>2}.2020')\n",
    "            \n",
    "        if day%5 == 0:\n",
    "            with open('fontanka_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "                json.dump(fontanka_links_2020, f)\n",
    "with open('fontanka_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(fontanka_links_2020, f)\n",
    "fontanka_links_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontanka_2020 = []\n",
    "for date in tqdm_notebook(sorted(list(fontanka_links_20.keys()))):\n",
    "    for link in tqdm_notebook(fontanka_links_20[date]):\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            soup = BS(r.text)\n",
    "            fontanka_2020.append({'Media':'fontanka.ru', 'Date':date, 'Link':link,\n",
    "                                  'Text':' '.join([p.text.strip() for p in soup.find('article').find_all('p') \n",
    "                                                   if '©' not in p.text]).replace('\\xa0', ' '), \n",
    "                                  'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(fontanka_2020).to_csv('fontanka_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(fontanka_2020).to_csv('fontanka_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(fontanka_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Moscow' Agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30, 7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\n",
    "moscow_20 = dict()\n",
    "for month in tqdm_notebook(list(months.keys())[1:2]):\n",
    "    for day in tqdm_notebook(list(range(months[month]))[8:]):\n",
    "        date = f'{day+1:02d}.{month:02d}.2020'\n",
    "        moscow_20[f'2020.{month:02d}.{day+1:02d}'] = []\n",
    "        page = 1\n",
    "        while True:\n",
    "            try:\n",
    "                url = f'https://www.mskagency.ru/?criteria=%D0%B8&from={date}&to={date}&page={page}&last_date={date}&rnd=1&list_only=1'\n",
    "                r = requests.get(url)\n",
    "                soup = BS(r.text)\n",
    "                links_temp = ['https://www.mskagency.ru'+a['href'] for a in soup.find_all('a', 'js-title')]\n",
    "                moscow_20[f'2020.{month:02d}.{day+1:02d}'].extend(links_temp)\n",
    "            except (AttributeError, IndexError):\n",
    "                print(date)\n",
    "                \n",
    "            if len(links_temp) < 50:\n",
    "                print(f'2020.{month:02d}.{day+1:02d}', len(moscow_20[f'2020.{month:02d}.{day+1:02d}']))\n",
    "                break\n",
    "            else:\n",
    "                page += 1\n",
    "                \n",
    "with open('moscow_links_2020.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(moscow_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moscow_2020 = []\n",
    "for date in tqdm_notebook(sorted(list(moscow_20.keys()))):\n",
    "    for link in tqdm_notebook(moscow_20[date]):\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            soup = BS(r.text)\n",
    "            moscow_2020.append({'Media':'mskagency.ru', 'Date':date, 'Link':link,\n",
    "                                  'Text':' '.join([soup.h1.text, ' '.join([p.text for p in soup.find('div', 'text').find_all('p')])]),\n",
    "                                 'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(moscow_2020).to_csv('moscow_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(moscow_2020).to_csv('moscow_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(moscow_2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mediazona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = set()\n",
    "for text in tqdm_notebook([\n",
    "#     '%D1%81%D0%BA%D0%B0%D0%B7%D0%B0%D0%BB', \n",
    "                           '%D0%B3%D0%BE%D0%B2%D0%BE%D1%80%D0%B8%D1%82',\n",
    "                          '%D1%81%D1%87%D0%B8%D1%82%D0%B0%D0%B5%D1%82', '%D1%80%D0%B0%D1%81%D1%81%D0%BA%D0%B0%D0%B7%D0%B0%D0%BB',\n",
    "                          '%D0%BD%D0%B0%D0%BF%D0%B8%D1%81%D0%B0%D0%BB', '%D1%81%D0%BE%D0%BE%D0%B1%D1%89%D0%B8%D0%BB',\n",
    "                          '%D0%BE%D0%B1%D1%8A%D1%8F%D1%81%D0%BD%D0%B8%D0%BB', '%D0%BF%D1%80%D0%B8%D1%88%D0%BB%D0%B8']):\n",
    "    page = 1\n",
    "    while True:\n",
    "        time.sleep(0.5)\n",
    "        link = f'https://zona.media/search?q={text}&page={page}'\n",
    "        soup = BS(requests.get(link).text)\n",
    "        links_temp = ['https://zona.media'+li.find('a')['href'] for li in soup.find_all('li', 'mz-search-result__item')]\n",
    "        links.update(links_temp)\n",
    "        print(page, len(links_temp), len(links))\n",
    "        if links_temp and '/2018/' not in links_temp[-1]:\n",
    "            page += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "with open('mediazona_links.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump({'data':list(links)}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = defaultdict(list)\n",
    "for link in tqdm_notebook(mediazona_links['data']):\n",
    "    if '/2020/' in link:\n",
    "        date = link[link.index('2020/'):link.index('2020/')+10].replace('/', '.')\n",
    "    if '/2019/' in link:\n",
    "        date = link[link.index('2019/'):link.index('2019/')+10].replace('/', '.')\n",
    "    links[date].append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediazona_19_20 = []\n",
    "for date in tqdm_notebook(sorted(list(links.keys()))):\n",
    "    for link in tqdm_notebook(links[date]):\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            soup = BS(r.text)\n",
    "            mediazona_19_20.append({'Media':'zona.media', 'Date':date, 'Link':link,\n",
    "                                  'Text':'. '.join([soup.find('header', 'mz-publish__title').text, \n",
    "                                                    ' '.join([p.text for p in \n",
    "                                                              soup.find('section', 'mz-publish__text').find_all('p')])]).replace('\\xa0', ' '), \n",
    "                                  'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(mediazona_19_20).to_csv('mediazona_19_20.csv', encoding='utf-8')\n",
    "pd.DataFrame(mediazona_19_20).to_csv('mediazona_19_20.csv', encoding='utf-8')\n",
    "pd.DataFrame(mediazona_19_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meduza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = set()\n",
    "for page in tqdm_notebook(range(30)):\n",
    "    for text in ['заявил', 'говорит', 'считает', 'Медуза', \n",
    "                     'рассказал', 'написал', 'сообщил', 'сообщает', \n",
    "                     'объяснил', 'попал', 'вышла', 'передает', 'пришли']:\n",
    "        time.sleep(1)\n",
    "        links.update(set(['https://meduza.io/'+link for link in \n",
    "                   requests.get(f'https://meduza.io/api/w5/search?term={text}&page={page}&per_page=100&locale=ru').json()['collection']\n",
    "                         if '2019' in link or '2020' in link]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meduza_links_2020 = defaultdict(set)\n",
    "\n",
    "for link in tqdm_notebook(list(links)):\n",
    "    if '/2020/' in link:\n",
    "        index = link.index('/2020/')\n",
    "        meduza_links_2020[link[index+1:index+11].replace('/', '.')].add(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meduza_2020 = []\n",
    "for date in tqdm_notebook(list(meduza_links_2020.keys())[27:]):\n",
    "    for link in tqdm_notebook(meduza_links_2020[date]):\n",
    "        if link not in pd.DataFrame(meduza_2020).Link:\n",
    "            try:\n",
    "                soup = BeautifulSoup(requests.get(link).text)\n",
    "                title = soup.h1.text\n",
    "                text = ' '.join([p.text for p in soup.find('div', 'GeneralMaterial-article') \n",
    "                                 if p.text and 'Напишите нам' not in p.text])\n",
    "\n",
    "                meduza_2020.append({'Media':'meduza.io', 'Date':date, 'Link':link,\n",
    "                                    'Text':' '.join([title, text]).replace('\\xa0', ' '), \n",
    "                                    'Dep':'', 'Tags':[]})\n",
    "            except (requests.exceptions.ConnectionError, AttributeError, IndexError, TypeError):\n",
    "                print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(meduza_2020).to_csv('meduza_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(meduza_2020).to_csv('meduza_2020.csv', encoding='utf-8')\n",
    "pd.DataFrame(meduza_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thebell_links = defaultdict(list)\n",
    "for page in tqdm_notebook(range(99, 410)):\n",
    "    link = f'https://thebell.io/category/news/page/{page}/'\n",
    "    r = requests.get(link)\n",
    "    soup = BS(r.text)\n",
    "    \n",
    "    for div in soup.find_all('div', 'news-item-row'):\n",
    "        if div.find('span'):\n",
    "            date = div.find('span', 'news-item__info').text.split()\n",
    "            date = f'{date[2]}.{months[date[1]]}.{date[0]}'\n",
    "            thebell_links[date].append(div.find('a', 'news-item__link')['href'])\n",
    "\n",
    "with open('thebell_links.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(thebell_links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thebell_19_20 = []\n",
    "for date in tqdm_notebook(sorted(list(thebell_links.keys()))):\n",
    "    for link in tqdm_notebook(thebell_links[date]):\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            soup = BS(r.text)\n",
    "            if len(date)==9:\n",
    "                date = date[:-1]+'0'+date[-1]\n",
    "            thebell_19_20.append({'Media':'thebell.io', 'Date':date, 'Link':link,\n",
    "                                  'Text':'. '.join([soup.h1.text, ' '.join([p.text for p in soup.article.find_all('p') \n",
    "                                                                            if p.text and 'Фото:' not in p.text][:-1])]), \n",
    "                                  'Dep':'', 'Tags':[]})\n",
    "        except (requests.exceptions.ConnectionError, AttributeError, \n",
    "                IndexError, requests.exceptions.TooManyRedirects, ValueError):\n",
    "            print(link)\n",
    "    if date.endswith('1'):\n",
    "        pd.DataFrame(thebell_19_20).to_csv('thebell_19_20.csv', encoding='utf-8')\n",
    "pd.DataFrame(thebell_19_20).to_csv('thebell_19_20.csv', encoding='utf-8')\n",
    "pd.DataFrame(thebell_19_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
